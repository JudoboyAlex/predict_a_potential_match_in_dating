{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Topic\n",
    "Is there a clear explanation of what this project is about? Does it state clearly which type of problem? E.g. type of learning and type of the task.\n",
    "Is the goal of the project clearly stated? E.g. why it’s important, what goal the author wants to achieve, or wants to learn.\n",
    "# Unlocking the Key to Finding Your Match\n",
    "\n",
    "## Introduction\n",
    "Struggling to find a match in the modern dating scene, I became determined to uncover the secret to success. My mom always assured me I was handsome, so looks couldn't be the issue. Could it be because I'm under 6 feet tall? My ethnicity? Or maybe my job title? Armed with my newfound knowledge of machine learning, I set out to dive deep into dating data and discover the hidden ingredients for finding my perfect match.\n",
    "\n",
    "## Project Topic\n",
    "Understanding the factors that lead to success on a second date can provide valuable insights into what I need to improve to find my ideal match. In this analysis, I employed several supervised machine learning algorithms, including logistic regression, random forest, and gradient boosting models, to predict the traits that make each gender more attractive for a match. The dataset used for this analysis includes various attributes related to dating. By training predictive models on this data, I aim not only to enhance my knowledge of machine learning but also to apply these findings to my personal quest for the perfect match.\n",
    "\n",
    "## About The Data\n",
    "The data I used is from Speed Dating dataset from Kaggle: https://www.kaggle.com/annavictoria/speed-dating-experiment\n",
    "The data was gathered from 552 participants in experimental speed dating events from 2002-2004.\n",
    "During the events, the attendees would have a four minute \"first date\" with every other participant of the opposite sex.\n",
    "At the end of their four minutes, participants were asked if they would like to see their date again. They were also asked to rate their date on six attributes:\n",
    "- Attractiveness\n",
    "- Sincerity\n",
    "- Intelligence\n",
    "- Fun\n",
    "- Ambition\n",
    "- Shared Interests.\n",
    "\n",
    "The dataset also includes questionnaire data gathered from participants at different points in the process. These fields include:\n",
    "- demographics\n",
    "- dating habits\n",
    "- self-perception across key attributes\n",
    "- beliefs on what others find valuable in a mate\n",
    "- lifestyle information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "pd.options.display.max_rows = 1000 #shows truncated results\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Look\n",
    "Conduct an initial assessment of the data type and its size. The speed dating data is found in a csv file(5.2 MB). See the speed-dating-data-key.doc for data dictionary and question key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>...</th>\n",
       "      <th>attr3_3</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iid   id  gender  idg  condtn  wave  round  position  positin1  order  ...  \\\n",
       "0    1  1.0       0    1       1     1     10         7       NaN      4  ...   \n",
       "1    1  1.0       0    1       1     1     10         7       NaN      3  ...   \n",
       "2    1  1.0       0    1       1     1     10         7       NaN     10  ...   \n",
       "3    1  1.0       0    1       1     1     10         7       NaN      5  ...   \n",
       "4    1  1.0       0    1       1     1     10         7       NaN      7  ...   \n",
       "\n",
       "   attr3_3  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  sinc5_3  intel5_3  \\\n",
       "0      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "1      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "2      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "3      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "4      5.0      7.0       7.0     7.0     7.0      NaN      NaN       NaN   \n",
       "\n",
       "   fun5_3  amb5_3  \n",
       "0     NaN     NaN  \n",
       "1     NaN     NaN  \n",
       "2     NaN     NaN  \n",
       "3     NaN     NaN  \n",
       "4     NaN     NaN  \n",
       "\n",
       "[5 rows x 195 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing data\n",
    "speed_dating_events_data = pd.read_csv('data/speed_dating_data.csv', encoding=\"ISO-8859-1\") # this encoding handles reading non-ASCII characters. \n",
    "speed_dating_events_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8378 entries, 0 to 8377\n",
      "Columns: 195 entries, iid to amb5_3\n",
      "dtypes: float64(174), int64(13), object(8)\n",
      "memory usage: 12.5+ MB\n"
     ]
    }
   ],
   "source": [
    "#To identify the data types and size of the data\n",
    "speed_dating_events_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    " There are 8378 rows of dating data with 195 columns. Many columns are resulted from 3 surveys asking participants same questions. The first survey, which was the survey filled out by students that are interested in participating in order to register for the event, was filled out completely which explains very few NaN in those columns. However, in subsequent follow up surveys, few participants responded which explains extensive amount of NaN in those columns.  \n",
    " Due to hundreds of columns, instead of dropping columns from original dataframe, I have decided to create multiple dataframes for analysis by extracting essential columns from original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iid            0\n",
       "id             1\n",
       "gender         0\n",
       "idg            0\n",
       "condtn         0\n",
       "wave           0\n",
       "round          0\n",
       "position       0\n",
       "positin1    1846\n",
       "order          0\n",
       "partner        0\n",
       "pid           10\n",
       "match          0\n",
       "int_corr     158\n",
       "samerace       0\n",
       "age_o        104\n",
       "race_o        73\n",
       "pf_o_att      89\n",
       "pf_o_sin      89\n",
       "pf_o_int      89\n",
       "pf_o_fun      98\n",
       "pf_o_amb     107\n",
       "pf_o_sha     129\n",
       "dec_o          0\n",
       "attr_o       212\n",
       "sinc_o       287\n",
       "intel_o      306\n",
       "fun_o        360\n",
       "amb_o        722\n",
       "shar_o      1076\n",
       "like_o       250\n",
       "prob_o       318\n",
       "met_o        385\n",
       "age           95\n",
       "field         63\n",
       "field_cd      82\n",
       "undergra    3464\n",
       "mn_sat      5245\n",
       "tuition     4795\n",
       "race          63\n",
       "imprace       79\n",
       "imprelig      79\n",
       "from          79\n",
       "zipcode     1064\n",
       "income      4099\n",
       "goal          79\n",
       "date          97\n",
       "go_out        79\n",
       "career        89\n",
       "career_c     138\n",
       "sports        79\n",
       "tvsports      79\n",
       "exercise      79\n",
       "dining        79\n",
       "museums       79\n",
       "art           79\n",
       "hiking        79\n",
       "gaming        79\n",
       "clubbing      79\n",
       "reading       79\n",
       "tv            79\n",
       "theater       79\n",
       "movies        79\n",
       "concerts      79\n",
       "music         79\n",
       "shopping      79\n",
       "yoga          79\n",
       "exphappy     101\n",
       "expnum      6578\n",
       "attr1_1       79\n",
       "sinc1_1       79\n",
       "intel1_1      79\n",
       "fun1_1        89\n",
       "amb1_1        99\n",
       "shar1_1      121\n",
       "attr4_1     1889\n",
       "sinc4_1     1889\n",
       "intel4_1    1889\n",
       "fun4_1      1889\n",
       "amb4_1      1889\n",
       "shar4_1     1911\n",
       "attr2_1       79\n",
       "sinc2_1       79\n",
       "intel2_1      79\n",
       "fun2_1        79\n",
       "amb2_1        89\n",
       "shar2_1       89\n",
       "attr3_1      105\n",
       "sinc3_1      105\n",
       "fun3_1       105\n",
       "intel3_1     105\n",
       "amb3_1       105\n",
       "attr5_1     3472\n",
       "sinc5_1     3472\n",
       "intel5_1    3472\n",
       "fun5_1      3472\n",
       "amb5_1      3472\n",
       "dec            0\n",
       "attr         202\n",
       "sinc         277\n",
       "intel        296\n",
       "fun          350\n",
       "amb          712\n",
       "shar        1067\n",
       "like         240\n",
       "prob         309\n",
       "met          375\n",
       "match_es    1173\n",
       "attr1_s     4282\n",
       "sinc1_s     4282\n",
       "intel1_s    4282\n",
       "fun1_s      4282\n",
       "amb1_s      4282\n",
       "shar1_s     4282\n",
       "attr3_s     4378\n",
       "sinc3_s     4378\n",
       "intel3_s    4378\n",
       "fun3_s      4378\n",
       "amb3_s      4378\n",
       "satis_2      915\n",
       "length       915\n",
       "numdat_2     945\n",
       "attr7_2     6394\n",
       "sinc7_2     6423\n",
       "intel7_2    6394\n",
       "fun7_2      6394\n",
       "amb7_2      6423\n",
       "shar7_2     6404\n",
       "attr1_2      933\n",
       "sinc1_2      915\n",
       "intel1_2     915\n",
       "fun1_2       915\n",
       "amb1_2       915\n",
       "shar1_2      915\n",
       "attr4_2     2603\n",
       "sinc4_2     2603\n",
       "intel4_2    2603\n",
       "fun4_2      2603\n",
       "amb4_2      2603\n",
       "shar4_2     2603\n",
       "attr2_2     2603\n",
       "sinc2_2     2603\n",
       "intel2_2    2603\n",
       "fun2_2      2603\n",
       "amb2_2      2603\n",
       "shar2_2     2603\n",
       "attr3_2      915\n",
       "sinc3_2      915\n",
       "intel3_2     915\n",
       "fun3_2       915\n",
       "amb3_2       915\n",
       "attr5_2     4001\n",
       "sinc5_2     4001\n",
       "intel5_2    4001\n",
       "fun5_2      4001\n",
       "amb5_2      4001\n",
       "you_call    4404\n",
       "them_cal    4404\n",
       "date_3      4404\n",
       "numdat_3    6882\n",
       "num_in_3    7710\n",
       "attr1_3     4404\n",
       "sinc1_3     4404\n",
       "intel1_3    4404\n",
       "fun1_3      4404\n",
       "amb1_3      4404\n",
       "shar1_3     4404\n",
       "attr7_3     6362\n",
       "sinc7_3     6362\n",
       "intel7_3    6362\n",
       "fun7_3      6362\n",
       "amb7_3      6362\n",
       "shar7_3     6362\n",
       "attr4_3     5419\n",
       "sinc4_3     5419\n",
       "intel4_3    5419\n",
       "fun4_3      5419\n",
       "amb4_3      5419\n",
       "shar4_3     5419\n",
       "attr2_3     5419\n",
       "sinc2_3     5419\n",
       "intel2_3    5419\n",
       "fun2_3      5419\n",
       "amb2_3      5419\n",
       "shar2_3     6362\n",
       "attr3_3     4404\n",
       "sinc3_3     4404\n",
       "intel3_3    4404\n",
       "fun3_3      4404\n",
       "amb3_3      4404\n",
       "attr5_3     6362\n",
       "sinc5_3     6362\n",
       "intel5_3    6362\n",
       "fun5_3      6362\n",
       "amb5_3      6362\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting null values\n",
    "speed_dating_events_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to notice that each participant is in the dataframe multiple times, once for each opposite gender participant. Thus an exploratory analysis on this dataset would be on 8378 individuals with many many repetitions.\n",
    "\n",
    "In other words, if I am Goku and I participate to a wave(speed dating event) with 10 participants of the opposite gender, I count as 10 Goku people. This can bias the analysis and therefore I want to create a second DataFrame with only the unique entries, giving us the real number of participants: 551."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_attributes = ['gender', 'age', 'field',\n",
    "       'race', 'imprace', 'imprelig', 'from',\n",
    "       'income', 'goal', 'date', 'go_out', 'career',\n",
    "       'career_c', 'sports', 'tvsports', 'exercise', 'dining', 'museums',\n",
    "       'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n",
    "       'movies', 'concerts', 'music', 'shopping', 'yoga', 'exphappy',\n",
    "       'expnum','match_es']\n",
    "decision = ['match','dec',\n",
    "       'attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob',\n",
    "       'met']\n",
    "evaluation = ['satis_2', 'length', 'numdat_2']\n",
    "outcome = ['you_call', 'them_cal', 'date_3', 'numdat_3',\n",
    "       'num_in_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speed_dating_events_analysis_data= speed_dating_events_data[['iid', 'wave'] + personal_attributes + evaluation + outcome].drop_duplicates().copy()\n",
    "len(speed_dating_events_analysis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>wave</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>field</th>\n",
       "      <th>race</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>from</th>\n",
       "      <th>income</th>\n",
       "      <th>...</th>\n",
       "      <th>expnum</th>\n",
       "      <th>match_es</th>\n",
       "      <th>satis_2</th>\n",
       "      <th>length</th>\n",
       "      <th>numdat_2</th>\n",
       "      <th>you_call</th>\n",
       "      <th>them_cal</th>\n",
       "      <th>date_3</th>\n",
       "      <th>numdat_3</th>\n",
       "      <th>num_in_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Law</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>69,487.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>law</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>65,929.00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Economics</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Law</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Texas</td>\n",
       "      <td>37,754.00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Law</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bowdoin College</td>\n",
       "      <td>86,340.00</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    iid  wave  gender   age      field  race  imprace  imprelig  \\\n",
       "0     1     1       0  21.0        Law   4.0      2.0       4.0   \n",
       "10    2     1       0  24.0        law   2.0      2.0       5.0   \n",
       "20    3     1       0  25.0  Economics   2.0      8.0       4.0   \n",
       "30    4     1       0  23.0        Law   2.0      1.0       1.0   \n",
       "40    5     1       0  21.0        Law   2.0      8.0       1.0   \n",
       "\n",
       "               from     income  ...  expnum  match_es  satis_2 length  \\\n",
       "0           Chicago  69,487.00  ...     2.0       4.0      6.0    2.0   \n",
       "10          Alabama  65,929.00  ...     5.0       3.0      5.0    2.0   \n",
       "20      Connecticut        NaN  ...     2.0       NaN      NaN    NaN   \n",
       "30            Texas  37,754.00  ...     2.0       2.0      4.0    3.0   \n",
       "40  Bowdoin College  86,340.00  ...    10.0       NaN      7.0    2.0   \n",
       "\n",
       "    numdat_2  you_call  them_cal  date_3  numdat_3  num_in_3  \n",
       "0        1.0       1.0       1.0     0.0       NaN       NaN  \n",
       "10       NaN       0.0       0.0     0.0       NaN       NaN  \n",
       "20       NaN       NaN       NaN     NaN       NaN       NaN  \n",
       "30       2.0       0.0       0.0     0.0       NaN       NaN  \n",
       "40       2.0       0.0       0.0     0.0       NaN       NaN  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speed_dating_events_analysis_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "### Who are the partipants?\n",
    "Here, we explore the dataset in terms of those features that describe the participants. The goals are to get to know what is in this dataset and get some hints on what to focus on for further analysis.\n",
    "\n",
    "These speed dating events were dedicated to partners of opposite genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender\n",
       "Male      277\n",
       "Female    274\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speed_dating_events_analysis_data['gender'] = speed_dating_events_analysis_data.gender.map({1 : 'Male', 0 : 'Female'}).fillna(speed_dating_events_analysis_data.gender)\n",
    "speed_dating_events_data['gender'] = speed_dating_events_data.gender.map({1 : 'Male', 0 : 'Female'}).fillna(speed_dating_events_data.gender)\n",
    "speed_dating_events_analysis_data.gender.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
